{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: Aamir Husain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch\n",
    "## Homework 2\n",
    "### Exercise 3\n",
    "\n",
    "-----\n",
    "\n",
    "### General instructions\n",
    "\n",
    "Complete the exercise listed below in this Jupyter notebook - leaving all of your code in Python cells in the notebook itself.  Feel free to add any necessary cells. \n",
    "\n",
    "### When submitting this homework:\n",
    "\n",
    "Make sure you have put your name at the top of each file\n",
    "    \n",
    "Make sure all output is present in your notebook prior to submission\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#a50e3e;\">Exercise 3. </span> Alternative form of logistic regression (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Section 9.1.2 of the class notes](https://jermwatt.github.io/mlrefined/blog_posts/9_Linear_twoclass_classification/9_1_Logistic_regression.html) we saw how the desire for having the following approximation\n",
    "for the $p^{th}$ data point $\\left(x_{p},y_{p}\\right)$ in Equation 7\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{tanh}\\left(y_{p}\\left(w_0+x_{p}w_1\\right)\\right)\\approx1\n",
    "\\end{equation}\n",
    "\n",
    "led us to forming the softmax perceptron cost function \n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(w_0, w_1\\right)=\\frac{1}{P}\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(w_0+x_{p}w_1\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "in Equation 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1.** Following a similar set of steps, show that the first equation above can be used to arrive at the related cost function given by\n",
    "\n",
    "\\begin{equation}\n",
    "h\\left(w_0, w_1\\right)=\\frac{1}{P}\\underset{p=1}{\\overset{P}{\\sum}}e^{-y_{p}\\left(w_0+x_{p}w_1\\right)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the exponential definition of tanh(), we know that:\n",
    "$$tanh(s)=\\frac{2}{1+e^{-s}}-1$$\n",
    "where $s=(y_p(w_0+x_pw_1))$, giving us\n",
    "$$1+e^{-y_p(w_0+x_pw_1)}=1$$\n",
    "Subtracting 1 from both sides yields\n",
    "$$e^{-y_p(w_0+x_pw_1)}=0$$\n",
    "We can now use this expression to define a softmax cost function\n",
    "\\begin{equation}\n",
    "h\\left(w_0, w_1\\right)=\\frac{1}{P}\\underset{p=1}{\\overset{P}{\\sum}}e^{-y_{p}\\left(w_0+x_{p}w_1\\right)}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.** We used both $g$ and $h$ to classify the toy two-class dataset shown below, resulting in two linear  classifiers. \n",
    "\n",
    "<p><img src=\"pics/two_classifiers.png\" width=\"35%\" height=\"auto\"></p>\n",
    "\n",
    "Determine which cost function was used in the training of the black classifier and which in the training of the purple one? Explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm having trouble picking up on any clues that point to which line could belong to which implementation, but here are my observations. The purple line seems to be more sensitive to outliers/noise - it looks like the singular red data point in the upper right is \"pulling\" the line closer to it. The black line, however, does not seem to be highly affected by outliers as it seperates the data much more cleanly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this is not a coding exercise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "121px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

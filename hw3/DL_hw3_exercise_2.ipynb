{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch\n",
    "## Homework 3\n",
    "### Exercise 2\n",
    "\n",
    "-----\n",
    "\n",
    "### General instructions\n",
    "\n",
    "Complete the exercise listed below in this Jupyter notebook - leaving all of your code in Python cells in the notebook itself.  Feel free to add any necessary cells. \n",
    "\n",
    "### When submitting this homework:\n",
    "\n",
    "Make sure you have put your name at the top of each file\n",
    "    \n",
    "Make sure all output is present in your notebook prior to submission\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#a50e3e;\">Exercise 2. </span>  Implement the ``maxout`` activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen various activation functions, most notably the tanh and relu functions defined below\n",
    "\n",
    "\\begin{array}\n",
    "\\\n",
    "a(x) = \\text{tanh}(w_0 + w_1x) \\\\\n",
    "a(x) = \\text{max}(0,w_0 + w_1x) \\\\\n",
    "\\end{array}\n",
    "\n",
    "In this exercise you will implement the so-called [*maxout* activation](https://arxiv.org/pdf/1302.4389.pdf).  This function, defined as\n",
    "\n",
    "\\begin{array}\n",
    "\\\n",
    "a(x) = \\text{max}(v_0 + v_1x, \\,w_0 + w_1x) \\\\\n",
    "\\end{array}\n",
    "\n",
    "this activation takes maximum of two linear combinations of the input, instead of one linear combination and zero like the relu function.  While this change is algebraically rather minor, multilayer perceptron architectures employing the *maxout* activation tends to have certain advantages over those employing tanh and relu activations, including\n",
    "\n",
    "- fewer issues with problematic initialization, e.g., values close too (or equal to) zero for the *relu* activation are bad because the relu is minimized at zero\n",
    "\n",
    "- fewer issues with gradients vanishing or exploding, as can occur when using the tanh activation\n",
    "\n",
    "- faster convergence with far fewer gradient descent steps\n",
    "\n",
    "These advantages come with a simple price: the maxout activation has twice as many internal parameters as either the relu or tanh, hence architectures built with them have roughly twice as many parameters to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this activation and appropriately adjust the corresponding  `Python` code so that you can build MLP feature transformations using the maxout activation.  The necessary code block adjustments include\n",
    "\n",
    "- changing the nonlinearity used in the function ``activation``\n",
    "\n",
    "- adjusting the `feature_transform` function that computes the MLP\n",
    "\n",
    "- adjusting the weight initialization module so that instead of creating one random weight matrix per hidden layer, you create and store two random weight matrices of identical size for every layer of the network\n",
    "\n",
    "and add this new functionality to your deep learning library `my_first_DL_lib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your code using a 5 layer input / activation output normalized architecture with 10 units in each layer, and fit this using a maximum of 200 steps of gradient descent to our prototype nonlinear regression dataset (the noisy sinusoidal dataset) - which you will find in the `datasets` directory.  Within this number of steps you should be able to achieve a very good fit to the dataset (so you will want to plot your fit after training!). Use a steplength parameter $\\alpha$ of the form $10^{-\\gamma}$ where $\\gamma$ is the smallest positive integer that produces convergence with your random initial set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should turn in:**\n",
    "    \n",
    "**1)** your adjusted code blocks\n",
    "    \n",
    "**2)** a cost function plot for your run of gradient descent that achieves the desired goal\n",
    "\n",
    "**3)** a plot showing your tuned model - using the best weights you found from your descent run - fit to the data (this model should overfit the given data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "143px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
